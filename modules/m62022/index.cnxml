<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>The Regression Equation -- BSTA 200 -- Humber College -- Version 2016RevA</title>
  <metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m62022</md:content-id>
  <md:title>The Regression Equation -- BSTA 200 -- Humber College -- Version 2016RevA</md:title>
  <md:abstract/>
  <md:uuid>f778fbca-6a69-4836-9866-24c769854630</md:uuid>
</metadata>

<content>
    <para id="eip-176">Regression analysis is a statistical technique that can test the hypothesis that a variable is dependent upon one or more other variables. Further, regression analysis can provide an estimate of the magnitude of the impact of a change in one variable on another. This last feature, of course, is all important in predicting future values.</para><para id="eip-851">Regression analysis is based upon a functional relationship among variables and further, assumes that the relationship is linear. When the relationship among variables is not linear, special techniques are required.  These techniques will not be discussed here.</para><para id="eip-80">The linear regression equation for two variables is as follows:</para><equation id="eip-773"><m:math><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover><m:mo>=</m:mo><m:mi>a</m:mi><m:mo>+</m:mo><m:mi>b</m:mi><m:mi>x</m:mi></m:math></equation><para id="eip-856">where <m:math><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:math> is the predicted value of <m:math><m:mi>y</m:mi></m:math> for a given <m:math><m:mi>x</m:mi></m:math> value, <m:math><m:mi>a</m:mi></m:math> is the <m:math><m:mi>y</m:mi></m:math> - intercept, <m:math><m:mi>b</m:mi></m:math> is the slope of the line, and <m:math><m:mi>x</m:mi></m:math> is any value of the independent variable.</para><para id="eip-610">The formula for the slope <m:math><m:mi>b</m:mi></m:math> is:</para><equation id="eip-846"><m:math><m:mi>b</m:mi><m:mo> </m:mo><m:mo>=</m:mo><m:mo> </m:mo><m:mfrac><m:mrow><m:mi>n</m:mi><m:mo>(</m:mo><m:mo>∑</m:mo><m:mi>x</m:mi><m:mi>y</m:mi><m:mo>)</m:mo><m:mo> </m:mo><m:mo>-</m:mo><m:mo> </m:mo><m:mo>(</m:mo><m:mo>∑</m:mo><m:mi>x</m:mi><m:mo>)</m:mo><m:mo>(</m:mo><m:mo>∑</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow><m:mrow><m:mi>n</m:mi><m:mo>(</m:mo><m:mo>∑</m:mo><m:msup><m:mi>x</m:mi><m:mn>2</m:mn></m:msup><m:mo>)</m:mo><m:mo> </m:mo><m:mo>-</m:mo><m:mo> </m:mo><m:mo>(</m:mo><m:mo>∑</m:mo><m:mi>x</m:mi><m:msup><m:mo>)</m:mo><m:mn>2</m:mn></m:msup></m:mrow></m:mfrac></m:math></equation><para id="eip-628">The formula for the <m:math><m:mi>y</m:mi></m:math> - intercept <m:math><m:mi>a</m:mi></m:math> is:</para><equation id="eip-364"><m:math><m:mi>a</m:mi><m:mo> </m:mo><m:mo>=</m:mo><m:mo> </m:mo><m:mfrac><m:mrow><m:mo>∑</m:mo><m:mi>y</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac><m:mo>-</m:mo><m:mi>b</m:mi><m:mfrac><m:mrow><m:mo>∑</m:mo><m:mi>x</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:math></equation><para id="fs-id1170588823276"><link target-id="fs-id1170579415184"/> presents the regression problem in the form of a scatter plot graph of the data set where it is hypothesized that <m:math><m:mi>y</m:mi></m:math> is dependent upon the single independent variable <m:math><m:mi>x</m:mi></m:math>.</para><figure id="fs-id1170579415184"><media id="eip-id8600088" alt="..."><image mime-type="image/jpeg" src="../../media/scatterDiagram2.png" print-width="3in" width="370"/></media></figure><para id="eip-802">In <link target-id="fs-id1170579415184"/> , the line that has been graphed through the points is the graph of the regression equation.  For this data, the regression equation is the line that best fits the data when plotted on a scatter diagram.  Notice that some points are above the line and some points are below the line.  The line has been chosen such that that the sum of the squares of the vertical distances from each point to the line is minimized.     </para><para id="eip-649">Consider a basic relationship from Macroeconomic Principle, the consumption function. This theoretical relationship states that as a person's income rises, their consumption rises, but by a smaller amount than the rise in income. We can explore this relationship through regression analysis.  In <link target-id="income_con"/> below, <m:math><m:mi>y</m:mi></m:math> is consumption and <m:math><m:mi>x</m:mi></m:math> is income. The regression problem is, first, to establish that this relationship exists, and second, to determine the impact of a change in income on a person's consumption. </para><para id="eip-130"><figure id="income_con"><media id="income" alt="Income versus consumption">
    <image mime-type="image/jpeg" src="../../media/incomeCons2.PNG" print-width="3in" width="370"/>
  </media>
  
</figure></para><para id="eip-509">Each "dot" in <link target-id="income_con"/> represents the consumption and income of different individuals at some point in time. </para><para id="eip-885">The regression problem comes down to determining which straight line would best represent the data in <link target-id="fs-id1165019842274"/>. Regression analysis is sometimes called "least squares" analysis because the method of determining which line best "fits" the data is to minimize the sum of the squared residuals of a line put through the data. </para><figure id="fs-id1165019842274"><media id="eip-id1167939682278" alt="..."><image mime-type="image/jpeg" src="../../media/fig-ch12_04_03m.jpg" print-width="3.5in" width="450"/></media></figure><para id="eip-977"><link target-id="fs-id1165019842274"/> shows the assumed relationship between consumption and income from macroeconomic theory. Here the data are plotted as a scatter plot and an estimated straight line has been drawn.  Each data point has error associated with it; that is, the distance from the point to the line of the regression equation.   We will see how by minimizing the sum of these errors we can get an estimate for the slope and intercept of this line.</para><para id="eip-870">Consider the graph below. The notation has returned to that for the more general model rather than the specific case of the Macroeconomic consumption function in our example.   </para><figure id="fs-id1165015413810"><media id="fs-id1165015399670" alt="..."><image mime-type="image/jpeg" src="../../media/resid.PNG" print-width="3.5in" width="500"/></media></figure><para id="eip-929">The <m:math><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:math> is read <emphasis>"<emphasis effect="italics">y</emphasis> -hat"</emphasis> and is the <emphasis>estimated value of <emphasis effect="italics">y</emphasis></emphasis>. (In <link target-id="fs-id1165019842274"/> <m:math><m:mover><m:mi>C</m:mi><m:mo stretchy="false">^</m:mo></m:mover></m:math> represents the estimated value of consumption because it is on the estimated line.) It is the value of <emphasis effect="italics">y</emphasis> obtained using the regression line. <m:math><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:math> is not generally equal to <m:math><m:mi>y</m:mi></m:math> from the data.</para><para id="eip-775">The term <m:math><m:mrow>
  <m:msub>
   <m:mi>y</m:mi>
   <m:mn>0</m:mn>
  </m:msub>
  <m:mo>−</m:mo><m:msub>
   <m:mover accent="true">
    <m:mi>y</m:mi>
    <m:mo>^</m:mo>
   </m:mover>
   
   <m:mn>0</m:mn>
  </m:msub>
  <m:mo>=</m:mo><m:msub>
   <m:mi>e</m:mi>
   <m:mn>0</m:mn>
  </m:msub>
  </m:mrow>
</m:math> is called the <emphasis>"error" or residual</emphasis>. It is not an error in the sense of a mistake. The error term was put into the estimating equation to capture missing variables and errors in measurement that may have occurred in the dependent variables. The <emphasis>absolute value of a residual</emphasis> measures the vertical distance between the actual value of <emphasis effect="italics">y</emphasis> and the estimated value of <emphasis effect="italics">y</emphasis>. In other words, it measures the vertical distance between the actual data point and the predicted point on the line as can be seen on the graph at point X<sub>0</sub>.</para><para id="eip-535">If the observed data point lies above the line, the residual is positive, and the line underestimates the actual data value for <emphasis effect="italics">y</emphasis>.</para><para id="eip-503">If the observed data point lies below the line, the residual is negative, and the line overestimates that actual data value for <emphasis effect="italics">y</emphasis>.</para><para id="eip-456">In the graph, <m:math>
 <m:mrow>
  <m:msub>
   <m:mi>y</m:mi>
   <m:mn>0</m:mn>
  </m:msub>
  <m:mo>−</m:mo><m:msub>
   <m:mover accent="true">
    <m:mi>y</m:mi>
    <m:mo>^</m:mo>
   </m:mover>
   
   <m:mn>0</m:mn>
  </m:msub>
  <m:mo>=</m:mo><m:msub>
   <m:mi>e</m:mi>
   <m:mn>0</m:mn>
  </m:msub>
  </m:mrow>
</m:math>
 is the residual for the point shown. Here the point lies above the line and the residual is positive.
For each data point the residuals, or errors, are calculated <m:math>
 <m:mrow>
  <m:msub>
   <m:mi>y</m:mi>
   <m:mi>i</m:mi>
  </m:msub>
  <m:mo>−</m:mo><m:msub>
   <m:mover accent="true">
    <m:mi>y</m:mi>
    <m:mo>^</m:mo>
   </m:mover>
   
   <m:mi>i</m:mi>
  </m:msub>
  <m:mo>=</m:mo><m:msub>
   <m:mi>e</m:mi>
   <m:mi>i</m:mi>
  </m:msub>
  </m:mrow>
</m:math>
 for i = 1, 2, 3, ..., n where n is the sample size. Each <m:math>
 <m:mrow>
  <m:mo>|</m:mo><m:msub>
   <m:mi>e</m:mi>
   <m:mi>i</m:mi>
  </m:msub>
  <m:mo>|</m:mo></m:mrow>
</m:math>
 is a vertical distance.

</para><para id="eip-63">The sum of the errors squared is the term obviously called <term>Sum of Squared Errors (SSE)</term>.</para><para id="eip-789">Using calculus, you can determine the straight line that has the parameter values of b<sub>0</sub> and b<sub>1</sub> that minimizes the <emphasis>SSE</emphasis>. When you make the <emphasis>SSE</emphasis> a minimum, you have determined the points that are on the line of best fit. This leads to the equations for <m:math><m:mi>a</m:mi></m:math> and <m:math><m:mi>b</m:mi></m:math> in the regression equation given above.</para><para id="eip-559"><title>Standard Error of Estimate</title>We can now find the <term>estimate of the error of estimate</term> which measures the dispersion around the mean. The formula for the standard error of estimate is:</para><equation id="eip-175"><m:math display="block">
 <m:mrow>
  <m:msub>
   <m:mi>s</m:mi>
   <m:mi>e</m:mi>
  </m:msub>
  <m:mo>=</m:mo><m:msqrt>
   <m:mrow>
    <m:mfrac>
     <m:mrow>
      <m:mstyle displaystyle="true">
       <m:mo>∑</m:mo> <m:mrow>
        <m:msup>
         <m:mi>y</m:mi>
         <m:mn>2</m:mn>
        </m:msup>
        </m:mrow>
      </m:mstyle><m:mo>−</m:mo><m:mi>a</m:mi><m:mrow><m:mo>(</m:mo>
       <m:mrow>
        <m:mstyle displaystyle="true">
         <m:mo>∑</m:mo> <m:mi>y</m:mi>
        </m:mstyle></m:mrow>
      <m:mo>)</m:mo></m:mrow><m:mo>−</m:mo><m:mi>b</m:mi><m:mrow><m:mo>(</m:mo>
       <m:mrow>
        <m:mstyle displaystyle="true">
         <m:mo>∑</m:mo> <m:mrow>
          <m:mi>x</m:mi><m:mi>y</m:mi></m:mrow>
        </m:mstyle></m:mrow>
      <m:mo>)</m:mo></m:mrow></m:mrow>
     <m:mrow>
      <m:mi>n</m:mi><m:mo>−</m:mo><m:mn>2</m:mn></m:mrow>
    </m:mfrac>
    </m:mrow>
  </m:msqrt>
  <m:mtext> </m:mtext><m:mtext> </m:mtext><m:mi>o</m:mi><m:mi>r</m:mi><m:mtext> </m:mtext><m:mtext> </m:mtext><m:msub>
   <m:mi>s</m:mi>
   <m:mi>e</m:mi>
  </m:msub>
  <m:mo>=</m:mo><m:msqrt>
   <m:mrow>
    <m:mfrac>
     <m:mrow>
      <m:mstyle displaystyle="true">
       <m:mo>∑</m:mo> <m:mrow>
        <m:msup>
         <m:mrow>
          <m:mrow><m:mo>(</m:mo>
           <m:mrow>
            <m:msub>
             <m:mi>y</m:mi>
             <m:mi>i</m:mi>
            </m:msub>
            <m:mo>−</m:mo><m:msub>
             <m:mover accent="true">
              <m:mi>y</m:mi>
              <m:mo>^</m:mo>
             </m:mover>
             
             <m:mi>i</m:mi>
            </m:msub>
            </m:mrow>
          <m:mo>)</m:mo></m:mrow></m:mrow>
         <m:mn>2</m:mn>
        </m:msup>
        </m:mrow>
      </m:mstyle></m:mrow>
     <m:mrow>
      <m:mi>n</m:mi><m:mo>−</m:mo><m:mn>2</m:mn></m:mrow>
    </m:mfrac>
    </m:mrow>
  </m:msqrt>
  </m:mrow>
</m:math>
</equation><para id="eip-27">This is a measure of the variance of the error terms and follows our regular standard deviation formula. One important note is that here we are dividing by <m:math><m:mo>(</m:mo><m:mi>n</m:mi><m:mo>-</m:mo><m:mi>2</m:mi><m:mo>)</m:mo></m:math>, which is the degrees of freedom. The degrees of freedom of a regression equation will be the number of observations, n, reduced by the number of estimated parameters, which includes the intercept as a parameter. </para><para id="eip-91">The standard error of estimate tells us just how “tight” the dispersion is about the line. As we will see shortly, the greater the dispersion about the line, meaning the larger the variance of the errors, the less probable that the hypothesized independent variable will be found to have a significant effect on the dependent variable. </para>




<section id="eip-759"><title>How Good is the Equation?</title><para id="eip-8">
In the last section we concerned ourselves with testing the hypothesis that the dependent variable did indeed depend upon the hypothesized independent variable or variables. It may be that we find an independent variable that has some effect on the dependent variable, but it may not be the only one, and it may not even be the most important one. Remember that the error term was placed in the model to capture the effects of any missing independent variables. It follows that the error term may be used to give a measure of the "goodness" of the equation taken as a whole in explaining the variation of the dependent variable, Y.
</para><para id="eip-42"><title>Coefficient of Determination</title>Another measure of the goodness of the regression equation is the <term>coefficient of determination</term>. It represents the proportion of variation in y that is <emphasis>explained</emphasis> by the regression on x, and is given by the formula:
<equation id="eip-id1167093457297"><m:math>
<m:msup><m:mi>R</m:mi><m:mn>2</m:mn></m:msup>
<m:mo>=</m:mo>
<m:mfrac>
<m:mi>SSR</m:mi>
<m:mi>SST</m:mi>
</m:mfrac>
</m:math>
</equation></para><para id="eip-268">where SSR is the regression sum of squares, the squared deviation of the predicted value of y from the mean value of y <m:math>
 <m:mrow>
  <m:mrow><m:mo>(</m:mo>
   <m:mrow>
    <m:msub>
     <m:mover accent="true">
      <m:mi>y</m:mi>
      <m:mo>^</m:mo>
     </m:mover>
     
     <m:mi>i</m:mi>
    </m:msub>
    <m:mo>−</m:mo><m:mover accent="true">
     <m:mi>y</m:mi>
     <m:mo>¯</m:mo>
    </m:mover>
    </m:mrow>
  <m:mo>)</m:mo></m:mrow></m:mrow>
</m:math>

,  and SST is the total sum of squares which is the total squared deviation of the dependent variable, y, from its mean value, including the error term. <link target-id="eip-id1172012809409"/> shows how the total deviation of the dependent variable, y, is partitioned into these two pieces. </para><figure id="eip-id1172012809409"><media id="eip-id1172008066777" alt="..."><image mime-type="image/jpeg" src="../../media/variation.PNG" print-width="4in" width="550"/></media></figure><para id="eip-146"><link target-id="eip-id1172012809409"/> shows the estimated regression line and a single observation, x<sub>i</sub>. Regression analysis tries to explain the variation of the data about the mean value of the dependent variable, y. The question is, why do the observations of y vary from the average level of y? The value of y at observation x<sub>i</sub> varies from the mean of y by the difference (<m:math><m:msub><m:mi>y</m:mi><m:mi>i</m:mi></m:msub><m:mo>−</m:mo><m:mover><m:mi>y</m:mi><m:mo>–</m:mo></m:mover></m:math>). The sum of these differences squared is SST, the sum of squares total. The actual value of y at x<sub>i</sub> deviates from the estimated value, ŷ, by the difference between the estimated value and the actual value, <m:math>
 <m:mrow>
  <m:mrow><m:mo>(</m:mo>
   <m:mrow>
    <m:msub>
     <m:mi>y</m:mi>
     <m:mi>i</m:mi>
    </m:msub>
    <m:mo>−</m:mo><m:msub>
     <m:mover accent="true">
      <m:mi>y</m:mi>
      <m:mo>^</m:mo>
     </m:mover>
     
     <m:mi>i</m:mi>
    </m:msub>
    </m:mrow>
  <m:mo>)</m:mo></m:mrow></m:mrow>
</m:math>
. We recall that this is the error term, e, and the sum of squares of these errors is SSE -sum of squared errors. The deviation of the predicted value of y, ŷ, from the mean value of y is <m:math>
 <m:mrow>
  <m:mrow><m:mo>(</m:mo>
   <m:mrow>
    <m:msub>
     <m:mover accent="true">
      <m:mi>y</m:mi>
      <m:mo>^</m:mo>
     </m:mover>
     
     <m:mi>i</m:mi>
    </m:msub>
    <m:mo>−</m:mo><m:mover accent="true">
     <m:mi>y</m:mi>
     <m:mo>¯</m:mo>
    </m:mover>
    </m:mrow>
  <m:mo>)</m:mo></m:mrow></m:mrow>
</m:math>
 and the sum of squares of these is the SSR -sum of squares regression. It is called “regression” because it is the deviation explained by the regression.</para><para id="eip-373">Because the SST = SSR + SSE we see that the multiple correlation coefficient is the percent of the variance, or deviation in y from its mean value, that is explained by the equation when taken as a whole. R<sup>2</sup> will vary between zero and 1, with zero indicating that none of the variation in y was explained by the equation, and a value of 1 indicating that 100% of the variation in y was explained by the equation. For time series studies expect a higher R<sup>2</sup>, and for cross-sectional data, expect lower R<sup>2</sup>. </para><para id="eip-651">While a high R<sup>2</sup> is desirable, remember that it is the tests of the hypothesis concerning the existence of a relationship between a set of independent variables and a particular dependent variable that was the motivating factor in using the regression model. It is validating a cause and effect relationship developed by some theory that is the true reason that we chose the regression analysis. Increasing the number of independent variables will have the effect of increasing R<sup>2</sup>. </para><para id="eip-113">There is no statistical test for the R<sup>2</sup> and thus little can be said about the model using R<sup>2</sup> with our characteristic confidence level. Two models that have the same size of SSE, that is sum of squared errors, may have very different R<sup>2</sup>  if the competing models have different SST, total sum of squared deviations. The goodness of fit of the two models is the same; they both have the same sum of squares unexplained, errors squared, but because of the larger total sum of squares on one of the models the R<sup>2</sup> differs. Again, the real value of regression as a tool is to examine hypotheses developed from a model that predicts certain relationships among the variables. These are tests of hypotheses on the coefficients of the model and not a game of maximizing R<sup>2</sup>.</para></section>




<section id="eip-933"><title>Testing the Parameters of the Line</title><para id="eip-726">The whole goal of the regression analysis was to test the hypothesis that the dependent variable, Y, was in fact dependent upon the values of the independent variables as asserted by some foundation theory, such as the consumption function example. Looking at the estimated equation under <link target-id="fs-id1165019842274"/>, we see that this amounts to determining the values of b<sub>0</sub> and b<sub>1</sub>. Notice that again we are using the convention of Greek letters for the population parameters and Roman letters for their estimates. 
</para><para id="eip-685">The regression analysis output provided by the computer software will produce an estimate of b<sub>0</sub> and b<sub>1</sub>, and any other b's for other independent variables that were included in the estimated equation. The issue is how good are these estimates? In order to test a hypothesis concerning any estimate, we have found that we need to know the underlying sampling distribution. It should come as no surprise at his stage in the course that the answer is going to be the normal distribution. This can be seen by remembering the assumption that the error term in the population, ε, is normally distributed. If the error term is normally distributed and the variance of the estimates of the equation parameters, b<sub>0</sub> and b<sub>1</sub>, are determined by the variance of the error term, it follows that the variances of the parameter estimates are also normally distributed. And indeed this is just the case. </para><para id="eip-531">We can see this by the creation of the test statistic for the test of hypothesis for the slope parameter, β<sub>1</sub> in our consumption function equation. To test whether or not Y does indeed depend upon X, or in our example, that consumption depends upon income, we need only test the hypothesis that β<sub>1</sub> equals zero. This hypothesis would be stated formally as:</para><equation id="eip-818"><m:math><m:msub><m:mi>H</m:mi><m:mn>0</m:mn></m:msub><m:mo>:</m:mo><m:msub><m:mi>β</m:mi><m:mn>1</m:mn></m:msub><m:mo>=</m:mo><m:mn>0</m:mn>
</m:math></equation><equation id="eip-906"><m:math>
<m:msub><m:mi>H</m:mi><m:mi>a</m:mi></m:msub><m:mo>:</m:mo><m:msub><m:mi>β</m:mi><m:mn>1</m:mn></m:msub><m:mo>≠</m:mo><m:mn>0</m:mn></m:math></equation><para id="eip-525">If we cannot reject the null hypothesis, we must conclude that our theory has no validity. If we cannot reject the null hypothesis that β<sub>1</sub> = 0 then b<sub>1</sub>, the coefficient of Income, is zero and zero times anything is zero. Therefore the effect of Income on Consumption is zero. There is no relationship as our theory had suggested. </para><para id="eip-349">Notice that we have set up the presumption, the null hypothesis, as "no relationship". This puts the burden of proof on the alternative hypothesis. In other words, if we are to validate our claim of finding a relationship, we must do so with a level of confidence greater than 90, 95, or 99 percent. The <emphasis effect="italics">status quo</emphasis> is ignorance, no relationship exists, and to be able to make the claim that we have actually added to our body of knowledge we must do so with significant probability of being 
correct. John Maynard Keynes got it right and thus was born Keynesian economics starting with this basic concept in 1936.
</para><para id="eip-28">The test statistic for this test comes directly from our old friend the standardizing formula:</para><equation id="eip-333"><m:math>
<m:msub>
 <m:mi>t</m:mi>
 <m:mi>c</m:mi>
</m:msub> 

<m:mo>=</m:mo> 

<m:mfrac> 
 <m:mrow> 
  <m:msub>
   <m:mi>b</m:mi>
   <m:mn>1</m:mn> 
  </m:msub>
  <m:mo>−</m:mo>
  <m:msub> 
   <m:mi>β</m:mi>
    <m:mn>1</m:mn> 
  </m:msub>
 </m:mrow>
 <m:mrow> 
   <m:msub> 
    <m:mi>S</m:mi> 
    <m:mrow>
    <m:msub> 
    <m:mi>b</m:mi>
    <m:mn>1</m:mn>
    </m:msub>
    </m:mrow>
  </m:msub>
 
</m:mrow>
</m:mfrac>
</m:math></equation><para id="eip-355">where b<sub>1</sub> is the estimated value of the slope of the regression line, β<sub>1</sub> is the hypothesized value of beta, in this case zero, and <m:math> <m:msub> 
    <m:mi>S</m:mi> 
    <m:mrow>
    <m:msub> 
    <m:mi>b</m:mi>
    <m:mn>1</m:mn>
    </m:msub>
    </m:mrow>
  </m:msub></m:math> is the standard deviation of the estimate of b<sub>1</sub>. In this case we are asking how many standard deviations is the estimated slope away from the hypothesized slope. This is exactly the same question we asked before with respect to a hypothesis about a mean: how many standard deviations is the estimated mean, the sample mean, from the hypothesized mean?</para><para id="eip-488">The test statistic is written as a student's t distribution, but if the sample size is larger enough so that the degrees of freedom are greater than 30 we may again use the normal distribution. To see why we can use the student's t or normal distribution we have only to look at <m:math> <m:msub> 
    <m:mi>S</m:mi> 
    <m:mrow>
    <m:msub> 
    <m:mi>b</m:mi>
    <m:mn>1</m:mn>
    </m:msub>
    </m:mrow>
  </m:msub></m:math>,the formula for the standard deviation of the estimate of b<sub>1</sub>:
</para><equation id="eip-786"><m:math>
 <m:msub>
 <m:mi>S</m:mi>
 <m:mrow>
 <m:msub>
 <m:mi>b</m:mi>
 <m:mn>1</m:mn>
 </m:msub>
 </m:mrow>
 </m:msub>

<m:mo>=</m:mo> 

<m:mfrac>
 <m:mrow> 
  <m:msubsup>
   <m:mi>S</m:mi>
   <m:mi>e</m:mi>
   <m:mn>2</m:mn>
  </m:msubsup>
 </m:mrow>
 <m:mrow>
  <m:msqrt>
   <m:msup>
   <m:mrow> 
   <m:mo>(</m:mo>
   <m:msub>
    <m:mi>x</m:mi>
    <m:mi>i</m:mi>
   </m:msub>
   <m:mo>−</m:mo>
   <m:mover>
   <m:mi>x</m:mi>
   <m:mi>–</m:mi>
   </m:mover> 
   <m:mo>)</m:mo>
   </m:mrow>
   <m:mn>2</m:mn>
   </m:msup> 
  </m:msqrt>
 </m:mrow>
</m:mfrac>  
</m:math>
 </equation><equation id="eip-288">or</equation><equation id="eip-99"><m:math>
 <m:msub>
 <m:mi>S</m:mi>
 <m:mrow>
 <m:msub>
 <m:mi>b</m:mi>
 <m:mn>1</m:mn>
 </m:msub>
 </m:mrow>
 </m:msub>

<m:mo>=</m:mo> 

<m:mfrac>
 <m:mrow> 
  <m:msubsup>
   <m:mi>S</m:mi>
   <m:mi>e</m:mi>
   <m:mn>2</m:mn>
  </m:msubsup>
 </m:mrow>
 <m:mrow>
  <m:mo>(</m:mo>
  <m:mi>n</m:mi>
  <m:mo>−</m:mo>
  <m:mn>1</m:mn>
  <m:mo>)</m:mo>
  <m:msubsup>
   <m:mi>S</m:mi>
   <m:mi>x</m:mi>
   <m:mn>2</m:mn>
  </m:msubsup>
 </m:mrow>
</m:mfrac>  
</m:math>
 </equation><para id="eip-679">Where S<sub>e</sub> is the estimate of the error variance and S<sup>2</sup><sub>x</sub> is the variance of x values of the coefficient of the independent variable being tested. </para><para id="eip-962">We see that S<sub>e</sub>, the <emphasis>estimate of the error variance</emphasis>, is part of the computation. Because the estimate of the error variance is based on the assumption of normality of the error terms, we can conclude that the sampling distribution of the beta's, the coefficients of our hypothesized regression line, are also normally distributed.</para><para id="eip-971">One last note concerns the degrees of freedom of the test statistic. Previously we subtracted 1 from the sample size to determine the degrees of freedom in a student's t problem.  Here we must subtract one degree of freedom for each parameter estimated in the equation. For the example of the consumption function we lose 2 degrees of freedom, one for <m:math><m:msub><m:mi>b</m:mi><m:mn>0</m:mn></m:msub></m:math>, the intercept, and one for b<sub>1</sub>, the slope of the consumption function. If we were estimating an equation with three independent variables, we would lose 4 degrees of freedom: three for the independent variables and one more for the intercept.</para><para id="eip-682">The decision rule for acceptance or rejection of the null hypothesis follows exactly the same form as in all our previous test of hypothesis. Namely, if the calculated value of t (or Z) falls into the tails of the distribution, where the tails are defined by α ,the required confidence in the test, we cannot accept the null hypothesis. If on the other hand, the calculated value of the test statistic is within the critical region, we cannot reject the null hypothesis.  </para><para id="eip-920">If we conclude that we cannot accept the null hypothesis, we are able to state with <m:math><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>α</m:mi><m:mo>)</m:mo></m:math> level of confidence that the slope of the line is given by b<sub>1</sub>. This is an extremely important conclusion. Regression analysis not only allows us to test if a cause and effect relationship exists, we can also determine the magnitude of that relationship, if one is found to exist. It is this feature of regression analysis that makes it so valuable. If models can be developed that have statistical validity, we are then able to simulate the effects of changes in variables that may be under our control with some degree of probability , of course. For example, if advertising is demonstrated to effect sales, we can determine the effects of changing the advertising budget and decide if the increased sales are worth the added expense.</para></section>






<section id="eip-993" class="summary"><title>Chapter Review</title><para id="eip-id1170036179985">It is hoped that this discussion of regression analysis has demonstrated the tremendous potential value it has as a tool for testing models and helping to better understand the world around us. The regression model has its limitations, especially the requirement that the underlying relationship be approximately linear.</para></section></content>
<glossary>
<definition id="fs-id1171342481921"><term>Residual or “error”</term>
<meaning id="fs-id1171348238510">the value calculated from subtracting 
<m:math><m:msub><m:mi>y</m:mi><m:mn>0</m:mn></m:msub>
<m:mo>−</m:mo>
<m:msub><m:mrow><m:mover><m:mi>y</m:mi><m:mo>^</m:mo></m:mover></m:mrow><m:mn>0</m:mn></m:msub>
<m:mo>=</m:mo>
<m:msub><m:mi>e</m:mi><m:mn>0</m:mn></m:msub></m:math>. The absolute value of a residual measures the vertical distance between the actual value of <emphasis effect="italics">y</emphasis> and the estimated value of <emphasis effect="italics">y</emphasis> that appears on the best-fit line.
</meaning>
</definition>

<definition id="fs-id1171345295516"><term>Sum of Squared Errors (SSE)</term>
<meaning id="fs-id1171346367514">the calculated value from adding up all the squared residual terms.  The hope is that this value is very small when creating a model.
</meaning>
</definition>

<definition id="fs-id4336325"><term><m:math><m:msup><m:mi>R</m:mi><m:mn>2</m:mn></m:msup></m:math> – Coefficient of Determination</term>
<meaning id="fs-id1171349427386">This is a number between 0 and 1 that represents the percentage variation of the dependent variable that can be explained by the variation in the independent variable.  Sometimes calculated by the equation <m:math><m:msup><m:mi>R</m:mi><m:mn>2</m:mn></m:msup><m:mo>=</m:mo><m:mfrac><m:mrow><m:mi>S</m:mi><m:mi>S</m:mi><m:mi>R</m:mi></m:mrow><m:mrow><m:mi>S</m:mi><m:mi>S</m:mi><m:mi>T</m:mi></m:mrow></m:mfrac></m:math> where SSR is the “Sum of Squares Regression” and SST is the “Sum of Squares Total.”


</meaning>
</definition>

</glossary>
</document>